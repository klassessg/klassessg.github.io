<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<!-- saved from url=(0060)http://www.icst.pku.edu.cn/course/icb/Projects/Hist-NLM.html -->
<HTML xmlns="http://www.w3.org/1999/xhtml">
<HEAD>
<META content="IE=5.0000" http-equiv="X-UA-Compatible">
<TITLE>Knight Owl: Robust Small Object Detection in the Dark</TITLE>
<META content="text/html; charset=utf-8" http-equiv=Content-Type>
<META name=language content=english><LINK rel=icon type=image/x-icon href="../../../favicon.ico">
<LINK rel=stylesheet type=text/css href="css/style.css">
<LINK rel=stylesheet type=text/css href="css/local.css">
<!-- <base target="_blank"> -->
<META name=GENERATOR content="MSHTML 9.00.8112.16496">
</HEAD>
<BODY>
<CENTER>
<DIV id=main>
<DIV id=pagetitle>Knight Owl: Robust Small Object Detection in the Dark</DIV><!-- #pagetitle -->
<DIV id=content>
<P><center>Contact:<A href="mailto:marketing@klasses.com.sg"><FONT color=#000080> marketing@klasses.com.sg</FONT></A></center></P>
  
<P>&nbsp;</P>


<H1><A name=index1h1></A>Introduction</H1>
<P>	We present a novel method to robustly detect objects in low-light. Our method is particularly more robust and accurate in detecting small objects which we define as objects of 10-20px (pixels) size. Please look at the qualitative and quantitative results below that highlight the superior performance of our method over the baselines, particularly in detecting small objects. We additionaly show that our method is not only accurate but also efficient and its runtime is on-par or even better than the competing baselines.</p>


<br>
  
<H1><A name=index2h1></A>Experimental Results</H1>
<p>Our method is implemented in PyTorch and tested on a single nVIDIA Tesla V100 GPU with a batch-size of 1 (similarly as the baselines). For training and testing our method as well as the baselines, we use the DarkFace [1] dataset that has an average object size of 11.09px. The input resolution is fixed to 640x640.

We compare our method with YOLOv7 [2] and MAET [3], state-of-the-art methods for general and low-light object detection respectively. We also compare our method with two-stage approach, Enlighten-GAN [4] + YOLOv7 [2], where we first enhance the input low-light image with state-of-the-art low-light enhancement method Enlighten-GAN [4], and then YOLOv7 [2] to generate the detection results on the enhanced image.</p>


<H2>Qualitative Evaluation:</H2>
<p>In the results below, green boxes and red boxes show the ground-truths and detection results/predictions respectively. Please see the highlighed regions (in yellow) to observe the significant areas of improvement shown by our method over the baselines.</p> 
 </br>
<div align="center">
  
	<table border="0" style="background-color:#FAFAFA; FONT-SIZE:24" >
	 <tr align="center">
		<td><img src="results/figs/8_real/2933_inp.jpg" alt="" height="240" ></td>
		<td><img src="results/figs/8_real/2933_ours.jpg" alt="" height="240" ></td>
		<td><img src="results/figs/8_real/2933_zYv7.jpg" alt="" height="240" ></td>
		<td><img src="results/figs/8_real/2933_maetYv7.jpg" alt="" height="240" ></td>
	 </tr>
	 <tr align="center">
		<td><img src="results/figs/8_real/5020_inp.jpg" alt="" height="240" ></td>
		<td><img src="results/figs/8_real/5020_ours.jpg" alt="" height="240" ></td>
		<td><img src="results/figs/8_real/5020_zYv7.jpg" alt="" height="240" ></td>
		<td><img src="results/figs/8_real/5020_maetYv7.jpg" alt="" height="240" ></td>
	 </tr>
	 <tr align="center">
		<td><img src="results/figs/8_real/4157_inp.jpg" alt="" height="240" ></td>
		<td><img src="results/figs/8_real/4157_ours.jpg" alt="" height="240" ></td>
		<td><img src="results/figs/8_real/4157_zYv7.jpg" alt="" height="240" ></td>
		<td><img src="results/figs/8_real/4157_maetYv7.jpg" alt="" height="240" ></td>
	 </tr>
	 <tr align="center">
		<td><img src="results/figs_supp/5_real/2979_inp.jpg" alt="" height="240" ></td>
		<td><img src="results/figs_supp/5_real/2979_ours.jpg" alt="" height="240" ></td>
		<td><img src="results/figs_supp/5_real/2979_zYv7.jpg" alt="" height="240" ></td>
		<td><img src="results/figs_supp/5_real/2979_maetYv7.jpg" alt="" height="240" ></td>
	 </tr>
	 <tr align="center">
		<td><img src="results/figs_supp/5_real/5840_inp.jpg" alt="" height="240" ></td>
		<td><img src="results/figs_supp/5_real/5840_ours.jpg" alt="" height="240" ></td>
		<td><img src="results/figs_supp/5_real/5840_zYv7.jpg" alt="" height="240" ></td>
		<td><img src="results/figs_supp/5_real/5840_maetYv7.jpg" alt="" height="240" ></td>
	 </tr>
	 <tr align="center">
		<td><img src="results/figs_supp/5_real/3103_inp.jpg" alt="" height="240" ></td>
		<td><img src="results/figs_supp/5_real/3103_ours.jpg" alt="" height="240" ></td>
		<td><img src="results/figs_supp/5_real/3103_zYv7.jpg" alt="" height="240" ></td>
		<td><img src="results/figs_supp/5_real/3103_maetYv7.jpg" alt="" height="240" ></td>
	 </tr>
	 <tr align="center">
		<td><img src="results/figs_supp/5_real/3806_inp.jpg" alt="" height="240" ></td>
		<td><img src="results/figs_supp/5_real/3806_ours.jpg" alt="" height="240" ></td>
		<td><img src="results/figs_supp/5_real/3806_zYv7.jpg" alt="" height="240" ></td>
		<td><img src="results/figs_supp/5_real/3806_maetYv7.jpg" alt="" height="240" ></td>
	 </tr>
	 <tr align="center">
		<td>(a) Input Image</td>
		<td>(b) Our Method</td>
		<td>(c) Enlighten-GAN[4] + YOLOv7[2]</td>
		<td>(d) MAET[3]</td>
	</tr>
	</table>
	<CENTER><P><EM>Qualitative comparisons on the DarkFace [1] dataset. The ground-truths are shown in green while the predictions are shown in red. As we can observe, our method generates the best detection results particularly in detecting small objects among all the competing methods (see the highlighted areas). Please zoom-in for better visualization.</EM></P></CENTER>
</div>

<br>
<br>

<H2>Quantitative Evaluation:</H2> 
<P>	<em>Table 1</em> shows the quantitative comparisons between our method and other existing methods: Eigen13 [1], Pix2Pix [2]. As shown in the table, compared to these two, our PSNR and SSIM values are higher. This indicates that our method can generate results more similar to the groundtruths.
We also compare our whole attentive GAN with parts of our own network: A (autoencoder alone without the attention map), A+D (non-attentive autoencoder plus non-attentive discriminator), A+AD (non-attentive autoencoder plus attentive discriminator). Note that, our whole attentive GAN can be written as AA+AD (attentive autoencoder plus attentive discriminator). As shown in the evaluation table, AA+AD performs better than the other possible configurations. This is the quantitative evidence that the attentive map is needed by both the generative and discriminative networks.
</P>
</br>
<div align="center">
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg .tg-nrw1{font-size:10px;text-align:center;vertical-align:top}
.tg .tg-3j8g{font-weight:bold;font-size:10px;text-align:center;vertical-align:top}
</style>
<table class="tg" style="undefined;table-layout: fixed; width: 488px">
<colgroup>
<col style="width: 70px">
<col style="width: 70px">
<col style="width: 70px">
<col style="width: 70px">
<col style="width: 70px">
</colgroup>
  <tr>
    <th class="tg-nrw1">Metric\Method</th>
    <th class="tg-nrw1">Eigen[1]</th>
    <th class="tg-nrw1">Pix2pix[2]</th>
    <th class="tg-nrw1">A</th>
	<th class="tg-nrw1">A+D</th>
	<th class="tg-nrw1">A+AD</th>
    <th class="tg-nrw1">Proposed</th>
  </tr>
  <tr>
    <td class="tg-nrw1">PSNR</td>
    <td class="tg-nrw1">28.59 </td>
    <td class="tg-nrw1">30.14 </td>
    <td class="tg-nrw1">29.25 </td>
	<td class="tg-nrw1">30.88 </td>
    <td class="tg-nrw1">30.60 </td>
    <td class="tg-3j8g">31.57 </td>
  </tr>
  <tr>
    <td class="tg-nrw1">SSIM</td>
    <td class="tg-nrw1">0.6726 </td>
    <td class="tg-nrw1">0.8299 </td>
    <td class="tg-nrw1">0.7853 </td>
	<td class="tg-nrw1">0.8670 </td>
	<td class="tg-nrw1">0.8710 </td>
    <td class="tg-3j8g">0.9023 </td>
  </tr>
</table>
<CENTER><P><EM>Table 1. PSNR(dB) and SSIM comparison of results
from different methods and possible configurations of our network. </EM></P></CENTER>
</div>

<br>
<br>

 <H1><A name=index6h1></A>References</H1>

<P>
	[1] Yang, W., Yuan, Y., Ren, W., Liu, J., Scheirer, W. J., Wang, Z., ... & Qin, L. (2020). Advancing image understanding in poor visibility environments: A collective benchmark study. IEEE Transactions on Image Processing, 29, 5737-5752.
</P>

<P>
	[2] Wang, C. Y., Bochkovskiy, A., & Liao, H. Y. M. (2023). YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 7464-7475).
</p>

<P>
	[3] Cui, Z., Qi, G. J., Gu, L., You, S., Zhang, Z., & Harada, T. (2021). Multitask aet with orthogonal tangent regularity for dark object detection. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 2553-2562).
</P>

<p>
	[4] Jiang, Y., Gong, X., Liu, D., Cheng, Y., Fang, C., Shen, X., ... & Wang, Z. (2021). Enlightengan: Deep light enhancement without paired supervision. IEEE transactions on image processing, 30, 2340-2349.
</p>