<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<!-- saved from url=(0060)http://www.icst.pku.edu.cn/course/icb/Projects/Hist-NLM.html -->
<HTML xmlns="http://www.w3.org/1999/xhtml">
<HEAD>
<META content="IE=5.0000" http-equiv="X-UA-Compatible">
<TITLE>Knight Owl: Robust Small Object Detection in the Dark</TITLE>
<META content="text/html; charset=utf-8" http-equiv=Content-Type>
<META name=language content=english><LINK rel=icon type=image/x-icon href="../../../favicon.ico">
<LINK rel=stylesheet type=text/css href="css/style.css">
<LINK rel=stylesheet type=text/css href="css/local.css">
<!-- <base target="_blank"> -->
<META name=GENERATOR content="MSHTML 9.00.8112.16496">
</HEAD>
<BODY>
<CENTER>
<DIV id=main>
<DIV id=pagetitle>Knight Owl: Robust Small Object Detection in the Dark</DIV><!-- #pagetitle -->
<DIV id=content>
<P><center>Contact:<A href="mailto:marketing@klasses.com.sg"><FONT color=#000080> marketing@klasses.com.sg</FONT></A></center></P>
  
<P>&nbsp;</P>


<H1><A name=index1h1></A>Introduction</H1>
<P>	We present a novel method to robustly detect objects in low-light. Our method is particularly more robust and accurate in detecting small objects which we define as objects of 10-20px (pixels) size. Please look at the qualitative and quantitative results below that highlight the superior performance of our method over the baselines, particularly in detecting small objects. We additionaly show that our method is not only accurate but also efficient, and its runtime is on-par or even better than the competing baselines.</p>


<br>
  
<H1><A name=index2h1></A>Experimental Results</H1>
<p>Our method is implemented in PyTorch and tested on a single nVIDIA Tesla V100 GPU with a batch-size of 1 (similarly as the baselines). In these experiments, for training and testing our method as well as the baselines, we use the DarkFace [1] dataset that has an average object size of 11.09px. The input resolution is fixed to 640x640.

We compare our method with YOLOv7 [2] and MAET [3], state-of-the-art methods for general and low-light object detection respectively. We also compare our method with a two-stage approach baseline, Enlighten-GAN [4] + YOLOv7 [2], where we first enhance the input low-light image with state-of-the-art low-light enhancement method Enlighten-GAN [4], and then use YOLOv7 [2] to generate the detection results on the enhanced image.</p>


<H2>Qualitative Evaluation:</H2>
<P>	<em>Fig. 1</em>: Qualitative comparisons on the DarkFace [1] dataset. The ground-truths are shown in green boxes while the detection results/predictions are shown in red boxes. As we can observe, our method generates the best detection results particularly in detecting small objects among all the competing methods (see the highlighted areas). Please zoom-in for better visualization. 
</P>
 </br>
<div align="center">
  
	<table border="0" style="background-color:#FAFAFA; FONT-SIZE:24" >
	 <tr align="center">
		<td><img src="results/figs/8_real/2933_inp.jpg" alt="" height="240" ></td>
		<td><img src="results/figs/8_real/2933_ours.jpg" alt="" height="240" ></td>
		<td><img src="results/figs/8_real/2933_zYv7.jpg" alt="" height="240" ></td>
		<td><img src="results/figs/8_real/2933_maetYv7.jpg" alt="" height="240" ></td>
	 </tr>
	 <tr align="center">
		<td><img src="results/figs/8_real/5020_inp.jpg" alt="" height="240" ></td>
		<td><img src="results/figs/8_real/5020_ours.jpg" alt="" height="240" ></td>
		<td><img src="results/figs/8_real/5020_zYv7.jpg" alt="" height="240" ></td>
		<td><img src="results/figs/8_real/5020_maetYv7.jpg" alt="" height="240" ></td>
	 </tr>
	 <tr align="center">
		<td><img src="results/figs/8_real/4157_inp.jpg" alt="" height="240" ></td>
		<td><img src="results/figs/8_real/4157_ours.jpg" alt="" height="240" ></td>
		<td><img src="results/figs/8_real/4157_zYv7.jpg" alt="" height="240" ></td>
		<td><img src="results/figs/8_real/4157_maetYv7.jpg" alt="" height="240" ></td>
	 </tr>
	 <tr align="center">
		<td><img src="results/figs_supp/5_real/2979_inp.jpg" alt="" height="240" ></td>
		<td><img src="results/figs_supp/5_real/2979_ours.jpg" alt="" height="240" ></td>
		<td><img src="results/figs_supp/5_real/2979_zYv7.jpg" alt="" height="240" ></td>
		<td><img src="results/figs_supp/5_real/2979_maetYv7.jpg" alt="" height="240" ></td>
	 </tr>
	 <tr align="center">
		<td><img src="results/figs_supp/5_real/5840_inp.jpg" alt="" height="240" ></td>
		<td><img src="results/figs_supp/5_real/5840_ours.jpg" alt="" height="240" ></td>
		<td><img src="results/figs_supp/5_real/5840_zYv7.jpg" alt="" height="240" ></td>
		<td><img src="results/figs_supp/5_real/5840_maetYv7.jpg" alt="" height="240" ></td>
	 </tr>
	 <tr align="center">
		<td><img src="results/figs_supp/5_real/3103_inp.jpg" alt="" height="240" ></td>
		<td><img src="results/figs_supp/5_real/3103_ours.jpg" alt="" height="240" ></td>
		<td><img src="results/figs_supp/5_real/3103_zYv7.jpg" alt="" height="240" ></td>
		<td><img src="results/figs_supp/5_real/3103_maetYv7.jpg" alt="" height="240" ></td>
	 </tr>
	 <tr align="center">
		<td><img src="results/figs_supp/5_real/3806_inp.jpg" alt="" height="240" ></td>
		<td><img src="results/figs_supp/5_real/3806_ours.jpg" alt="" height="240" ></td>
		<td><img src="results/figs_supp/5_real/3806_zYv7.jpg" alt="" height="240" ></td>
		<td><img src="results/figs_supp/5_real/3806_maetYv7.jpg" alt="" height="240" ></td>
	 </tr>
	 <tr align="center">
		<td>(a) Input Image</td>
		<td>(b) Our Method</td>
		<td>(c) Enlighten-GAN[4] + YOLOv7[2]</td>
		<td>(d) MAET[3]</td>
	</tr>
	</table>
</div>

<br>
<br>

<H2>Quantitative Evaluation:</H2> 
<P>	<em>Table 1</em>: Quantitative comparisons on the DarkFace [1] dataset between our method and the existing baselines. Performance metrics mAP (s<10), mAP(s<20), and mAP respresent the mean aveage precision for objects with size less than 10px, objects with size less than 20px, and for all object sizes respectively. Runtime numbers are in ms/image. 
</P>
</br>
<div align="center">
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:20px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg th{font-family:Arial, sans-serif;font-size:20px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg .tg-nrw1{font-size:20px;text-align:center;vertical-align:top}
.tg .tg-3j8g{font-weight:bold;font-size:20px;text-align:center;vertical-align:top}
</style>
<table class="tg" style="undefined;table-layout: fixed; width: 1100px">
<colgroup>
<col style="width: 200px">
<col style="width: 200px">
<col style="width: 300px">
<col style="width: 200px">
</colgroup>
  <tr>
    <th class="tg-nrw1">Metric/Method</th>
    <th class="tg-nrw1">YOLOv7 [2]</th>
    <th class="tg-nrw1">MAET [3]</th>
    <th class="tg-nrw1">Enlighten-GAN [4] + YOLOv7 [2</th>
    <th class="tg-3j8g">Our Method</th>
  </tr>
  <tr>
    <td class="tg-nrw1">mAP (s<10)</td>
    <td class="tg-nrw1">0.3407 </td>
    <td class="tg-nrw1">0.2643 </td>
    <td class="tg-nrw1">0.1009 </td>
    <td class="tg-3j8g">0.4777 </td>
  </tr>
  <tr>
    <td class="tg-nrw1">mAP (s<20)</td>
    <td class="tg-nrw1">0.5458 </td>
    <td class="tg-nrw1">0.4576 </td>
    <td class="tg-nrw1">0.1952 </td>
    <td class="tg-3j8g">0.6429 </td>
  </tr>
  <tr>
    <td class="tg-nrw1">mAP </td>
    <td class="tg-nrw1">0.6599 </td>
    <td class="tg-nrw1">0.5619 </td>
    <td class="tg-nrw1">0.3548 </td>
    <td class="tg-3j8g">0.7325 </td>
  </tr>
  <tr>
    <td class="tg-nrw1">Runtime </td>
    <td class="tg-nrw1">~23.9ms </td>
    <td class="tg-nrw1">~23.4ms </td>
    <td class="tg-nrw1">~32.4ms </td>
    <td class="tg-nrw1">~24.2ms </td>
  </tr>
</table>
</div>

<br>

 <H1><A name=index6h1></A>References</H1>

<P>
	[1] Yang, W., Yuan, Y., Ren, W., Liu, J., Scheirer, W. J., Wang, Z., ... & Qin, L. (2020). Advancing image understanding in poor visibility environments: A collective benchmark study. IEEE Transactions on Image Processing, 29, 5737-5752.
</P>

<P>
	[2] Wang, C. Y., Bochkovskiy, A., & Liao, H. Y. M. (2023). YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 7464-7475).
</p>

<P>
	[3] Cui, Z., Qi, G. J., Gu, L., You, S., Zhang, Z., & Harada, T. (2021). Multitask aet with orthogonal tangent regularity for dark object detection. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 2553-2562).
</P>

<p>
	[4] Jiang, Y., Gong, X., Liu, D., Cheng, Y., Fang, C., Shen, X., ... & Wang, Z. (2021). Enlightengan: Deep light enhancement without paired supervision. IEEE transactions on image processing, 30, 2340-2349.
</p>