<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<!-- saved from url=(0060)http://www.icst.pku.edu.cn/course/icb/Projects/Hist-NLM.html -->
<HTML xmlns="http://www.w3.org/1999/xhtml">
<HEAD>
<META content="IE=5.0000" http-equiv="X-UA-Compatible">
<TITLE>Knight Owl: Robust Small Object Detection in the Dark</TITLE>
<META content="text/html; charset=utf-8" http-equiv=Content-Type>
<META name=language content=english><LINK rel=icon type=image/x-icon href="../../../favicon.ico">
<LINK rel=stylesheet type=text/css href="css/style.css">
<LINK rel=stylesheet type=text/css href="css/local.css">
<!-- <base target="_blank"> -->
<META name=GENERATOR content="MSHTML 9.00.8112.16496">
</HEAD>
<BODY>
<CENTER>
<DIV id=main>
<DIV id=pagetitle>Knight Owl: Robust Small Object Detection in the Dark</DIV><!-- #pagetitle -->
<DIV id=content>
<P><center>Contact:<A href="mailto:marketing@klasses.com.sg"><FONT color=#000080> marketing@klasses.com.sg</FONT></A></center></P>
  
<P>&nbsp;</P>


<H1><A name=index1h1></A>Introduction</H1>
<P>	Raindrops adhered to a glass window or camera lens can severely hamper the visibility of a background scene and degrade an image considerably. In this paper, we address the problem by visually removing raindrops, and thus transforming a raindrop degraded image into a clean one. The problem is intractable, since first the regions occluded by raindrops are not given. Second, the information about the background scene of the occluded regions is completely lost for most part. To resolve the problem, we apply an attentive generative network using adversarial training. Our main idea is to inject visual attention into both the generative and discriminative networks. During the training, our visual attention learns about raindrop regions and their surroundings. Hence, by injecting this information, the generative network will pay more attention to the raindrop regions and the surrounding structures, and the discriminative network will be able to assess the local consistency of the restored regions. This injection of visual attention to both generative and discriminative networks is the main contribution of this paper. Our experiments show the effectiveness of our approach, which outperforms the state of the art methods quantitatively and qualitatively.</p>


<br>
<br>
<br>
  
<H1><A name=index2h1></A>Experimental Results</H1>
<p>The proposed method is implemented by PyTorch on four Nvidia Titan-XP GPUs. 
We compare our method with Eigen’s method [1],
Pix2pix-cGAN [2], You.et al.'s video based raindrop removal method [3, 4, 5].</p>


<H2>Qualitative Evaluation:</H2> 
 </br>
<div align="center">
  
	<table border="0" style="background-color:#FAFAFA; FONT-SIZE:24" >
	 <tr align="center">
		<td><img src="sup/more/0_0.png" alt="" height="110" ></td>
		<td><img src="sup/more/0_1.png" alt="" height="110" ></td>
		<td><img src="sup/more/0_2.png" alt="" height="110" ></td>
		<td><img src="sup/more/0_3.png" alt="" height="110" ></td>
		<td><img src="sup/more/0_4.png" alt="" height="110" ></td>
	 </tr>
	 <tr align="center">
		<td><img src="sup/more/1_0.png" alt="" height="110" ></td>
		<td><img src="sup/more/1_1.png" alt="" height="110" ></td>
		<td><img src="sup/more/1_2.png" alt="" height="110" ></td>
		<td><img src="sup/more/1_3.png" alt="" height="110" ></td>
		<td><img src="sup/more/1_4.png" alt="" height="110" ></td>
	 </tr>
	 <tr align="center">
		<td><img src="sup/more/2_0.png" alt="" height="110" ></td>
		<td><img src="sup/more/2_1.png" alt="" height="110" ></td>
		<td><img src="sup/more/2_2.png" alt="" height="110" ></td>
		<td><img src="sup/more/2_3.png" alt="" height="110" ></td>
		<td><img src="sup/more/2_4.png" alt="" height="110" ></td>
	 </tr>
	 <tr align="center">
		<td><img src="sup/more/20_1.png" alt="" height="110" ></td>
		<td><img src="sup/more/20_0.png" alt="" height="110" ></td>
		<td><img src="sup/more/20_2.png" alt="" height="110" ></td>
		<td><img src="sup/more/20_4.png" alt="" height="110" ></td>
		<td><img src="sup/more/20_5.png" alt="" height="110" ></td>
	 </tr>
	 <tr align="center">
		<td><img src="sup/more/4_0.png" alt="" height="110" ></td>
		<td><img src="sup/more/4_1.png" alt="" height="110" ></td>
		<td><img src="sup/more/4_2.png" alt="" height="110" ></td>
		<td><img src="sup/more/4_3.png" alt="" height="110" ></td>
		<td><img src="sup/more/4_4.png" alt="" height="110" ></td>
	 </tr>
	 <tr align="center">
		<td><img src="sup/more/21_1.png" alt="" height="110" ></td>
		<td><img src="sup/more/21_0.png" alt="" height="110" ></td>
		<td><img src="sup/more/21_2.png" alt="" height="110" ></td>
		<td><img src="sup/more/21_4.png" alt="" height="110" ></td>
		<td><img src="sup/more/21_5.png" alt="" height="110" ></td>
	 </tr>
	 <tr align="center">
		<td><img src="sup/more/5_0.png" alt="" height="110" ></td>
		<td><img src="sup/more/5_1.png" alt="" height="110" ></td>
		<td><img src="sup/more/5_2.png" alt="" height="110" ></td>
		<td><img src="sup/more/5_3.png" alt="" height="110" ></td>
		<td><img src="sup/more/5_4.png" alt="" height="110" ></td>
	 </tr>
	 <tr align="center">
		<td>(a)Ground Truth</td>
		<td>(b)Raindrop Image</td>
		<td>(c)Eigen [1]</td>
		<td>(d)Pix2pix [2]</td>
		<td>(e)Our Method</td>
	</tr>
	</table>
	<br/>
	<CENTER><P><EM>Fig.2 Results of comparing a few different methods. From left to right: ground truth, raindrop image (input), Eigen13 [1], Pix2Pix [2] and our method. Nearly all raindrops are removed by our method despite the diversity of their colors, shapes and transparency.</EM></P></CENTER>
</div>

<br>
<br>

<H2>Quantitative Evaluation:</H2> 
<P>	<em>Table 1</em> shows the quantitative comparisons between our method and other existing methods: Eigen13 [1], Pix2Pix [2]. As shown in the table, compared to these two, our PSNR and SSIM values are higher. This indicates that our method can generate results more similar to the groundtruths.
We also compare our whole attentive GAN with parts of our own network: A (autoencoder alone without the attention map), A+D (non-attentive autoencoder plus non-attentive discriminator), A+AD (non-attentive autoencoder plus attentive discriminator). Note that, our whole attentive GAN can be written as AA+AD (attentive autoencoder plus attentive discriminator). As shown in the evaluation table, AA+AD performs better than the other possible configurations. This is the quantitative evidence that the attentive map is needed by both the generative and discriminative networks.
</P>
</br>
<div align="center">
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg .tg-nrw1{font-size:10px;text-align:center;vertical-align:top}
.tg .tg-3j8g{font-weight:bold;font-size:10px;text-align:center;vertical-align:top}
</style>
<table class="tg" style="undefined;table-layout: fixed; width: 488px">
<colgroup>
<col style="width: 70px">
<col style="width: 70px">
<col style="width: 70px">
<col style="width: 70px">
<col style="width: 70px">
</colgroup>
  <tr>
    <th class="tg-nrw1">Metric\Method</th>
    <th class="tg-nrw1">Eigen[1]</th>
    <th class="tg-nrw1">Pix2pix[2]</th>
    <th class="tg-nrw1">A</th>
	<th class="tg-nrw1">A+D</th>
	<th class="tg-nrw1">A+AD</th>
    <th class="tg-nrw1">Proposed</th>
  </tr>
  <tr>
    <td class="tg-nrw1">PSNR</td>
    <td class="tg-nrw1">28.59 </td>
    <td class="tg-nrw1">30.14 </td>
    <td class="tg-nrw1">29.25 </td>
	<td class="tg-nrw1">30.88 </td>
    <td class="tg-nrw1">30.60 </td>
    <td class="tg-3j8g">31.57 </td>
  </tr>
  <tr>
    <td class="tg-nrw1">SSIM</td>
    <td class="tg-nrw1">0.6726 </td>
    <td class="tg-nrw1">0.8299 </td>
    <td class="tg-nrw1">0.7853 </td>
	<td class="tg-nrw1">0.8670 </td>
	<td class="tg-nrw1">0.8710 </td>
    <td class="tg-3j8g">0.9023 </td>
  </tr>
</table>
<CENTER><P><EM>Table 1. PSNR(dB) and SSIM comparison of results
from different methods and possible configurations of our network. </EM></P></CENTER>
</div>

<br>
<br>

 <H1><A name=index6h1></A>References</H1>

<P>
	[1] D. Eigen, D. Krishnan, and R. Fergus. Restoring an image taken through a window covered with dirt or rain. In Proceedings of the IEEE International Conference on Computer Vision, pages 633–640, 2013.
</P>

<P>
	[2] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image translation with conditional adversarial networks. arXiv preprint arXiv:1611.07004, 2016.
</p>

<P>
	[3] S. You, R. T. Tan, R. Kawakami, and K. Ikeuchi. Adherent raindrop detection and removal in video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1035–1042, 2013.
</P>

<p>
	[4] S. You, R. T. Tan, R. Kawakami, Y. Mukaigawa, and K. Ikeuchi. Raindrop detection and removal from long range
trajectories. In Asian Conference on Computer Vision, pages 569–585. Springer, 2014.
</p>

<p>
	[5] S. You, R. T. Tan, R. Kawakami, Y. Mukaigawa, and K. Ikeuchi. Adherent raindrop modeling, detectionand removal in video. IEEE transactions on pattern analysis and machine intelligence, 38(9):1721–1733, 2016.
</p>

<p>
	[6] S. You. http://users.cecs.anu.edu.au/ ̃shaodi.you/CVPR2013/Shaodi_CVPR2013.html.
</p>



